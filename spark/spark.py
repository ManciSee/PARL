# from __future__ import print_function
# import sys
# import os
# from pyspark import SparkContext, SparkConf
# from pyspark.sql import SparkSession
# from pyspark.streaming import StreamingContext
# from pyspark.sql.dataframe import DataFrame
# from elasticsearch import Elasticsearch
# import csv
# import json
# from pyspark.sql.functions import from_json
# from pyspark.sql.types import StructType, StructField, StringType
# import pandas as pd

# def elaborate(batch_df: DataFrame, batch_id: int):
#     batch_df.show(truncate=False)

# # Create a Spark context and session
# SparkConf = SparkConf().set("es.nodes", "elasticsearch").set("es.port", "9200")
# sc = SparkContext(conf=SparkConf, appName="StreamingKafka")
# spark = SparkSession(sc)
# sc.setLogLevel("ERROR")

# # Kafka
# kafkaServer = "kafka:39092"
# topic = "speech"

# # Elasticsearch
# elastic_host = "http://elasticsearch:9200"
# elastic_index = "speech"

# es_mapping = {
#     "mappings": {
#         "properties": {
#             "timestamp": {
#                 "type": "date",
#                 "format": "yyyy-MM-dd'T'HH:mm:ss"
#             },
#             "text": {
#                 "type": "text",
#                 "fielddata": True
#             },
#             "duration": {
#                 "type": "float"
#             }
#         }
#     }
# }



# schema = StructType([
#     StructField("id", StringType(), True),
#     StructField("timestamp", StringType(), True),
#     StructField("text", StringType(), True),
#     StructField("duration", StringType(), True)
# ])

# # def write_to_csv(record):
# #     csv_file_path = "/app/transcription.csv"
# #     column_names = ["id", "timestamp", "text", "duration"]

# #     if not os.path.isfile(csv_file_path):
# #         with open(csv_file_path, 'w', newline='') as f:
# #             writer = csv.writer(f)
# #             writer.writerow(column_names)
    
# #     with open(csv_file_path, 'a', newline='') as f:
# #         writer = csv.writer(f)
# #         writer.writerow([record['id'], record['timestamp'], record['text'], record['duration']])

# def write_to_csv_and_send_to_es(record):
#     csv_file_path = "/app/transcription.csv"
#     column_names = ["id", "timestamp", "text", "duration"]

#     if not os.path.isfile(csv_file_path):
#         with open(csv_file_path, 'w', newline='') as f:
#             writer = csv.writer(f)
#             writer.writerow(column_names)
    
#     with open(csv_file_path, 'a', newline='') as f:
#         writer = csv.writer(f)
#         writer.writerow([record['id'], record['timestamp'], record['text'], record['duration']])
    
#     # Create Elasticsearch client inside the function
#     es = Elasticsearch(elastic_host)
    
#     # Send data to Elasticsearch
#     es_data = {
#         "id": record['id'],
#         "timestamp": record['timestamp'],
#         "text": record['text'],
#         "duration": record['duration']
#     }
#     es.index(index=elastic_index, body=es_data, ignore=400)

# print("Reading from Kafka...")
# df = spark \
#     .readStream \
#     .format("kafka") \
#     .option("kafka.bootstrap.servers", kafkaServer) \
#     .option("subscribe", topic) \
#     .load()

# df = df.selectExpr("CAST(value AS STRING)")
# df = df.select(from_json("value", schema).alias("data"))
# df = df.select("data.id", "data.timestamp", "data.text", "data.duration")

# print("Save to CSV and send to Elasticsearch...")
# df.writeStream \
#     .foreach(write_to_csv_and_send_to_es) \
#     .start() \
#     .awaitTermination()
# print("Done!")


#--------------------------------------------------------------------------------------------------------------------
from __future__ import print_function
import sys
import os
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession
from pyspark.streaming import StreamingContext
from pyspark.sql.dataframe import DataFrame
from elasticsearch import Elasticsearch
import csv
import json
from pyspark.sql.functions import from_json
from pyspark.sql.types import StructType, StructField, StringType
import pandas as pd
from gensim import corpora
from gensim.models import LdaModel
from gensim.parsing.preprocessing import preprocess_string
import re
import numpy as np

def elaborate(batch_df: DataFrame, batch_id: int):
    batch_df.show(truncate=False)

# Create a Spark context and session
SparkConf = SparkConf().set("es.nodes", "elasticsearch").set("es.port", "9200")
sc = SparkContext(conf=SparkConf, appName="StreamingKafka")
spark = SparkSession(sc)
sc.setLogLevel("ERROR")

# Kafka
kafkaServer = "kafka:39092"
topic = "speech"

# Elasticsearch
elastic_host = "http://elasticsearch:9200"
elastic_index = "speech"

es_mapping = {
    "mappings": {
        "properties": {
            "timestamp": {
                "type": "date",
                "format": "yyyy-MM-dd'T'HH:mm:ss"
            },
            "text": {
                "type": "text",
                "fielddata": True
            },
            "duration": {
                "type": "float"
            }
        }
    }
}


schema = StructType([
    StructField("id", StringType(), True),
    StructField("timestamp", StringType(), True),
    StructField("text", StringType(), True),
    StructField("duration", StringType(), True)
])

def write_to_csv_and_send_to_es(record):
    csv_file_path = "/app/transcription.csv"
    column_names = ["id", "timestamp", "text", "duration"]

    if not os.path.isfile(csv_file_path):
        with open(csv_file_path, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(column_names)
    
    # with open(csv_file_path, 'a', newline='') as f:
    #     writer = csv.writer(f)
    #     writer.writerow([record['id'], record['timestamp'], record['text'], record['duration']])
    
    # Create Elasticsearch client inside the function
    es = Elasticsearch(elastic_host)

    #Â topic modelling
    dataset = pd.read_csv(csv_file_path)

    np.random.seed(42)

    dictionary = corpora.Dictionary()

    results = []

    print("Finding topic...")
    # Itera su ogni riga del dataset
    for index, row in dataset.iterrows():
        document = preprocess_string(row['text'])
        dictionary.add_documents([document])

        bow = dictionary.doc2bow(document)
        lda_model = LdaModel([bow], num_topics=3, id2word=dictionary, passes=15)

        topics = lda_model[bow]
        dominant_topic = max(topics, key=lambda x: x[1])
        topic_index, topic_score = dominant_topic

        top_terms = [term for term, _ in lda_model.show_topic(topic_index, topn=3) if not (term.isdigit() or '*' in term)]

        results.append({
            'ID': row['id'],
            'Topic': topic_index,
            'Score': topic_score,
            'Top Terms': ' , '.join(top_terms)
        })
    print("Done!")

    es_data = {
        "id": record['id'],
        "timestamp": record['timestamp'],
        "text": record['text'],
        "duration": record['duration'],
        "topic": topic_index,
        "score": topic_score,
        "top_terms": ', '.join(top_terms)
    }
    es.index(index=elastic_index, body=es_data, ignore=400)

    print("Send data and topics to es!")

print("Reading from Kafka...")
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafkaServer) \
    .option("subscribe", topic) \
    .load()

df = df.selectExpr("CAST(value AS STRING)")
df = df.select(from_json("value", schema).alias("data"))
df = df.select("data.id", "data.timestamp", "data.text", "data.duration")

print("Saving to CSV and sending to Elasticsearch...")
df.writeStream \
    .foreach(write_to_csv_and_send_to_es) \
    .start() \
    .awaitTermination()